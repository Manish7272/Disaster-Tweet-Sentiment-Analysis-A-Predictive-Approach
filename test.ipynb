{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Manish Gupta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Manish\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable does not exist\n",
      "variable does not exist\n",
      "variable does not exist\n"
     ]
    }
   ],
   "source": [
    "var_name = [\"acronyms_dict\", \"contractions_dict\", \"stops\"]\n",
    "for var in var_name:\n",
    "    if var in locals():\n",
    "        print(\"variable exists\")\n",
    "    else:\n",
    "        print(\"variable does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    acronyms_dict, contractions_dict, stops\n",
    "except NameError:\n",
    "    acronyms_dict = pd.read_json(\"acronym.json\", typ = \"series\")\n",
    "    contractions_dict = pd.read_json(\"contractions.json\", typ = \"series\")\n",
    "    stops = list(pd.read_csv('stopwords.csv').values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining tokenizer\n",
    "regexp = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    text = text.lower()                                                                                        # lowercase\n",
    "    text = text.strip()                                                                                        # whitespaces\n",
    "    \n",
    "    # Removing html tags\n",
    "    html = re.compile(r'<.*?>')\n",
    "    text = html.sub(r'', text)                                                                                 # html tags\n",
    "    \n",
    "    # Removing emoji patterns\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)                                                                         # unicode char\n",
    "    \n",
    "    # Removing urls\n",
    "    http = \"https?://\\S+|www\\.\\S+\" # matching strings beginning with http (but not just \"http\")\n",
    "    pattern = r\"({})\".format(http) # creating pattern\n",
    "    text = re.sub(pattern, \"\", text)                                                                            # remove urls\n",
    "    \n",
    "    # Removing twitter usernames\n",
    "    pattern = r'@[\\w_]+'\n",
    "    text = re.sub(pattern, \"\", text)                                                                            # remove @twitter usernames\n",
    "    \n",
    "    # Removing punctuations and numbers\n",
    "    punct_str = string.punctuation + string.digits\n",
    "    punct_str = punct_str.replace(\"'\", \"\")\n",
    "    punct_str = punct_str.replace(\"-\", \"\")\n",
    "    text = text.translate(str.maketrans('', '', punct_str))                                                     # punctuation and numbers\n",
    "    \n",
    "    # Replacing \"-\" in text with empty space\n",
    "    text = text.replace(\"-\", \" \")                                                                               # \"-\"\n",
    "    \n",
    "    # Substituting acronyms\n",
    "    words = []\n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in acronyms_dict.index:\n",
    "            words = words + acronyms_dict[word].split()\n",
    "        else:\n",
    "            words = words + word.split()\n",
    "    text = ' '.join(words)                                                                                       # acronyms\n",
    "    \n",
    "    # Substituting Contractions\n",
    "    words = []\n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in contractions_dict.index:\n",
    "            words = words + contractions_dict[word].split()\n",
    "        else:\n",
    "            words = words + word.split()\n",
    "    text = \" \".join(words)                                                                                       # contractions\n",
    "    \n",
    "    punct_str = string.punctuation\n",
    "    text = text.translate(str.maketrans('', '', punct_str))                                                     # punctuation again to remove \"'\"\n",
    "    \n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in regexp.tokenize(text)])                             # lemmatize\n",
    "    \n",
    "    # Stopwords Removal\n",
    "    text = ' '.join([word for word in regexp.tokenize(text) if word not in stops])                              # stopwords\n",
    "    \n",
    "    # Removing all characters except alphabets and \" \" (space)\n",
    "    filter = string.ascii_letters + \" \"\n",
    "    text = \"\".join([chr for chr in text if chr in filter])                                                      # remove all characters except alphabets and \" \" (space)\n",
    "    \n",
    "    # Removing words with one alphabet occuring more than 3 times continuously\n",
    "    pattern = r'\\b\\w*?(.)\\1{2,}\\w*\\b'\n",
    "    text = re.sub(pattern, \"\", text).strip()                                                                    # remove words with one alphabet occuring more than 3 times continuously\n",
    "    \n",
    "    # Removing words with less than 3 characters\n",
    "    short_words = r'\\b\\w{1,2}\\b'\n",
    "    text = re.sub(short_words, \"\", text)                                                                     # remove words with less than 3 characters\n",
    "    \n",
    "    # return final output\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat car'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"cat behind car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Manish Gupta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:107: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Manish Gupta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:1113: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"model_transfer_USE\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " reshape_3 (Reshape)         (None, 1, 512)            0         \n",
      "                                                                 \n",
      " lstm_12 (LSTM)              (None, 1, 64)             147712    \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 1, 64)             0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256981185 (980.31 MB)\n",
      "Trainable params: 183361 (716.25 KB)\n",
      "Non-trainable params: 256797824 (979.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = tf.keras.models.load_model(\"transfer_tweet\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9986302]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict([preprocess(\"Hamas bombs hospital, 2000 dead, catastrophic earthquakes\")])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.squeeze(tf.round(y_pred)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
